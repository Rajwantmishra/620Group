{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment Detail:\n",
    "\n",
    "For this project, please work with the entire class as one collaborative group! Your project should be submitted (as a Jupyter Notebook via GitHub) by end of the due date. The group should present their code and findings in our meetup. The ability to be an effective member of a virtual team is highly valued in the data science job market.\n",
    "\n",
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can.\n",
    "Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the devtest set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. \n",
    "\n",
    "How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
    "\n",
    "Source: Natural Language Processing with Python, exercise 6.10.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Building data set: Using nltk function we have build gender data set called \"Gender_names\" here. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\rajwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import random\n",
    "from nltk.classify import apply_features\n",
    "\n",
    "#Building the Gender_names data set\n",
    "Gender_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(Gender_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Avivah', 'female'),\n",
       " ('Emmaline', 'female'),\n",
       " ('Rori', 'female'),\n",
       " ('Shannon', 'male'),\n",
       " ('Celia', 'female'),\n",
       " ('Gay', 'female'),\n",
       " ('Charin', 'female'),\n",
       " ('Pollyanna', 'female'),\n",
       " ('Brena', 'female'),\n",
       " ('Tobye', 'female')]"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_names[0:10] #show the names with gender. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Identification:\n",
    "Male and female names have distinct characteristics such as names ending in a, e, and i are likely to be female, while names ending in k, o, r, s, and t are likely to be male. We have build a classifier to model these differences more precisely. We will look for the last letter of a given name. (Source: NLP book page 222-223) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'e'}"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('Justine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have used the feature extractor to process the Gender_names data, and divide the resulting list of feature sets into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with a to z letters check :0.786\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), g) for (n,g) in Gender_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "accuracy_base= nltk.classify.accuracy(classifier, test_set)\n",
    "print (\"Accuracy with a to z letters check :{}\".format(accuracy_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'last_letter': 'i'}, 'female')"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "print (classifier.classify(gender_features('Romeo'))) #male\n",
    "print (classifier.classify(gender_features('Trinity'))) #female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     33.9 : 1.0\n",
      "             last_letter = 'k'              male : female =     32.7 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.6 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.2 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LastLetter</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LastLetter  Gender\n",
       "0          h  female\n",
       "1          e  female\n",
       "2          i  female\n",
       "3          n    male\n",
       "4          a  female"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gf = pd.DataFrame(featuresets, columns=['Letter','Gender'])\n",
    "df_gf['LastLetter'] = df_gf['Letter'].apply(lambda x: x['last_letter'])\n",
    "df_gf = df_gf[['LastLetter','Gender']]\n",
    "df_gf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7944, 2)"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with large corpora, we will use  use the function\n",
    "nltk.classify.apply_features which does not store all the feature sets in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO WE need THIS CODE\n",
    "train_set = apply_features(gender_features, Gender_names[500:])\n",
    "test_set = apply_features(gender_features, Gender_names[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the Right Features:\n",
    "Selecting relevant features and deciding how to encode them are very important to build a  good model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features_az(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender_features2('John')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of naive Bayes classifier using the feature extractor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Last Letter  [classifier] check :0.786\n",
      "Accuracy First and Last Letter [classifier_az] check :0.8\n"
     ]
    }
   ],
   "source": [
    "featuresets_az = [(gender_features_az(n), g) for (n,g) in Gender_names]\n",
    "train_set, test_set = featuresets_az[500:], featuresets_az[:500]\n",
    "classifier_az = nltk.NaiveBayesClassifier.train(train_set)\n",
    "accuracy_az= nltk.classify.accuracy(classifier_az, test_set)\n",
    "print (\"Accuracy with Last Letter  [classifier] check :{}\".format(accuracy_base))\n",
    "print (\"Accuracy First and Last Letter [classifier_az] check :{}\".format(accuracy_az))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'firstletter': 'e', 'lastletter': 'e', 'count(a)': 1, 'has(a)': True, 'count(b)': 0, 'has(b)': False, 'count(c)': 0, 'has(c)': False, 'count(d)': 0, 'has(d)': False, 'count(e)': 2, 'has(e)': True, 'count(f)': 0, 'has(f)': False, 'count(g)': 0, 'has(g)': False, 'count(h)': 0, 'has(h)': False, 'count(i)': 1, 'has(i)': True, 'count(j)': 0, 'has(j)': False, 'count(k)': 0, 'has(k)': False, 'count(l)': 1, 'has(l)': True, 'count(m)': 2, 'has(m)': True, 'count(n)': 1, 'has(n)': True, 'count(o)': 0, 'has(o)': False, 'count(p)': 0, 'has(p)': False, 'count(q)': 0, 'has(q)': False, 'count(r)': 0, 'has(r)': False, 'count(s)': 0, 'has(s)': False, 'count(t)': 0, 'has(t)': False, 'count(u)': 0, 'has(u)': False, 'count(v)': 0, 'has(v)': False, 'count(w)': 0, 'has(w)': False, 'count(x)': 0, 'has(x)': False, 'count(y)': 0, 'has(y)': False, 'count(z)': 0, 'has(z)': False}, 'female')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(featuresets_az[1])\n",
    "list(featuresets_az[1][0].values())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Duo</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ah</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ee</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ri</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sn</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Duo  Gender\n",
       "0  ah  female\n",
       "1  ee  female\n",
       "2  ri  female\n",
       "3  sn    male\n",
       "4  ca  female"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gf_az = pd.DataFrame(featuresets_az, columns=['Letter','Gender'])\n",
    "df_gf_az['Duo'] = df_gf_az['Letter'].apply(lambda x: list(x.values())[0] + list(x.values())[1]) \n",
    "df_gf_az = df_gf_az[['Duo','Gender']]\n",
    "df_gf_az.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7944, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_gf_az.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above result shows that the accuracy of classifier to count letters is about 2% more\n",
    "than the accuracy of a classifier that only pays attention to the final letter of each name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis\n",
    "\n",
    "First, we select a development set, containing the corpus data for creating the model. This development set is then subdivided into the *training set* and the *dev-test* set.\n",
    "<br>\n",
    "<br>\n",
    "**devtest_names :** Unique records from 500 to 1500 Index  <br>\n",
    "**train_names :** Unique records from 1500+ Index <br>\n",
    "**test_names :** Unique records from 0 to 500 Index <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = Gender_names[1500:]\n",
    "devtest_names = Gender_names[500:1500]\n",
    "test_names = Gender_names[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have divided the corpus into appropriate datasets. Then we have built a model using the training\n",
    "set, and then run it on the dev-test set.\n",
    "\n",
    "#### Running Base Gender Classifier with Last Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features(n), g) for (n,g) in devtest_names]\n",
    "test_set = [(gender_features(n), g) for (n,g) in test_names]\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier1, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Gender Classifier with First and Last Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features_az(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features_az(n), g) for (n,g) in devtest_names]\n",
    "test_set = [(gender_features_az(n), g) for (n,g) in test_names]\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier2, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dev-test set, we can generate a list of the errors that the classifier makes when\n",
    "predicting name genders:<br>\n",
    "Lets use our 500 Devtest_name to check the predictions by using the both Classifier1  with Feature method : gender_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier1.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )\n",
    "\n",
    "errors = sorted(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names classifier that we have built generates about **216 errors** on the **devtest_names** corpus as follows, we are listing few of them as below : <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=female   guess=male     name=Allsun                        \n",
      "correct=female   guess=male     name=Alys                          \n",
      "correct=female   guess=male     name=Arden                         \n",
      "correct=female   guess=male     name=Ardis                         \n",
      "correct=female   guess=male     name=Astrid                        \n",
      "correct=female   guess=male     name=Avis                          \n",
      "correct=female   guess=male     name=Beatriz                       \n",
      "correct=female   guess=male     name=Bette-Ann                     \n",
      "correct=female   guess=male     name=Bev                           \n",
      "correct=female   guess=male     name=Bridget                       \n"
     ]
    }
   ],
   "source": [
    "for (tag, guess, name) in errors[0:10]: # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    print ('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that <b>l </b>is  mostly know as Male but, <b>el</b> can be classified as Female.\n",
    "<br>Similarly We note that <b>n</b>is  mostly know as Male but, <b>nn/an</b> can be classified as Female.\n",
    "\n",
    "We will now build another model where we would consider the last two letters of the word and then train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect last Two Letters from the Words \n",
    "def gender_features_tls(word):\n",
    "    return {'suffix1': word[-1:], 'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix1': 'n', 'suffix2': 'hn'}"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features_lt(\"John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features_tls(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features_tls(n), g) for (n,g) in devtest_names]\n",
    "classifier3 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier3, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuilding the classifier with the new feature extractor, we see that the performance\n",
    "on the dev-test dataset improves by almost two percentage points from 78.4% to 80.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created another features here which utilizes the first and last letter. It also looks for the prefix and suffix, or first and last two or three letters, depending on the name's length of a name and looks for whether or not any of the consonant clusters are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED SOURCE FOR cons_clusters\n",
    "def class_gender_features4(name):\n",
    "    features = {}\n",
    "    temp_name = name\n",
    "    cons_clusters = [\"bl\", \"br\", \"ch\", \"cl\", \"cr\", \"dr\", \"fl\", \"fr\", \n",
    "                     \"gl\", \"gr\", \"pl\", \"pr\", \"sc\", \"sh\", \"sk\", \"sl\", \n",
    "                     \"sm\", \"sn\", \"sp\", \"st\", \"sw\", \"th\", \"tr\", \"tw\", \n",
    "                     \"wh\", \"wr\", \"sch\", \"scr\", \"shr\", \"sph\", \"spl\", \n",
    "                     \"spr\", \"squ\", \"str\", \"thr\"]\n",
    "    features[\"firstletter\"] = name[0].lower() \n",
    "    features[\"lastletter\"] = name[-1].lower() \n",
    "    features[\"prefix\"] = name[:3].lower() if len(name) > 4 else name[:2].lower() \n",
    "    features[\"suffix\"] = name[-3:].lower() if len(name) > 4 else name[-2:].lower()\n",
    "    clusters = []\n",
    "    for cluster in cons_clusters[::-1]:\n",
    "        if cluster in temp_name:\n",
    "            temp_name = temp_name.replace(cluster, \"\")\n",
    "            clusters.append(cluster)\n",
    "    features[\"consonant_clusters_1\"] = clusters[0] if len(clusters) > 0 else None\n",
    "    features[\"consonant_clusters_2\"] = clusters[1] if len(clusters) > 1 else None\n",
    "    features[\"consonant_clusters_3\"] = clusters[2] if len(clusters) > 2 else None\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'firstletter': 'r',\n",
       " 'lastletter': 'n',\n",
       " 'prefix': 'raj',\n",
       " 'suffix': 'wan',\n",
       " 'consonant_clusters_1': None,\n",
       " 'consonant_clusters_2': None,\n",
       " 'consonant_clusters_3': None}"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_gender_features4(\"RAJWAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              lastletter = 'a'            female : male   =     34.8 : 1.0\n",
      "              lastletter = 'k'              male : female =     18.9 : 1.0\n",
      "                  suffix = 'nne'          female : male   =     18.9 : 1.0\n",
      "                  suffix = 'ita'          female : male   =     15.3 : 1.0\n",
      "                  suffix = 'tta'          female : male   =     14.4 : 1.0\n",
      "              lastletter = 'o'              male : female =     12.7 : 1.0\n",
      "                  suffix = 'ard'            male : female =     12.6 : 1.0\n",
      "                  suffix = 'and'            male : female =     11.5 : 1.0\n",
      "                  suffix = 'son'            male : female =     11.1 : 1.0\n",
      "                  prefix = 'dor'          female : male   =     10.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_4.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING with new Model with Vowel Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def gender_features_icv(name):\n",
    "    features = {}\n",
    "    temp_name = name.lower()\n",
    "    lenName = len(name)\n",
    "    if lenName >= 4 :\n",
    "        features[\"name_len\"] = 1 \n",
    "    \n",
    "    features[\"name_len\"] = len(name)\n",
    "    features[\"firstletter\"] = name[0].lower() \n",
    "    features[\"lastletter\"] = name[-1].lower() \n",
    "    features[\"prefix\"] = name[:3].lower() if len(name) > 4 else name[:2].lower() \n",
    "    features[\"suffix\"] = name[-3:].lower() if len(name) > 4 else name[-2:].lower()\n",
    "    Vowel = ['a','e','i','o','u']\n",
    "    Vclusters = []\n",
    "    dataSetI = []\n",
    "    dataSetII = []\n",
    "#     Check if Last 2 Letters are Vowels\n",
    "    if name[-2:lenName-1] in Vowel :  features[\"Vowel_l2\"] =  1  \n",
    "    if name[-1:lenName] in Vowel : features[\"Vowel_l1\"] =  1\n",
    "        #     Find the Cosine Distance of Last 2 Letters \n",
    "        #    temp_name[0],temp_name[1],temp_name[-2],temp_name[-1]\n",
    "    if lenName >= 4 :      \n",
    "        dataSetI = [\n",
    "                ord(name[0])-96,ord(name[1])-96,ord(name[-2])-96,ord(name[-3])-96] # This returns Number of Letters\n",
    "        dataSetII = [#ord(name[-1:lenName])-96,ord(name[-1].lower())-96,\n",
    "                 ord(name[1])-96,ord(name[2])-96,ord(name[-1])-96,ord(name[-2])-96]  # This returns Number of Letters\n",
    "    \n",
    "    else:\n",
    "        dataSetI = [\n",
    "                ord(name[0])-96,ord(name[-2])-96] # This returns Number of Letters\n",
    "        dataSetII = [#ord(name[-1:lenName])-96,ord(name[-1].lower())-96,\n",
    "                 ord(name[1])-96,ord(name[-1])-96]  # This returns Number of Letters\n",
    "    \n",
    "    cos_result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "    features[\"cos_lt\"] = cos_result\n",
    "    features[\"Len_vowel\"] = 0  \n",
    "    dataSetI = []\n",
    "    dataSetII = []\n",
    "    flag = False\n",
    "    for vb in Vowel[::-1]:\n",
    "        if vb in temp_name:\n",
    "            n_vowels= temp_name.count(vb) # COunt how many times you see Vowels            \n",
    "            temp_name = temp_name.replace(vb, \"\")\n",
    "            Vclusters.append(vb)\n",
    "            features[vb]=n_vowels\n",
    "            features[\"Len_vowel\"] = features[\"Len_vowel\"] + n_vowels\n",
    "            if flag == True:\n",
    "                dataSetI.append(ord(vb)-96)\n",
    "                flag = False\n",
    "            else :\n",
    "                dataSetII.append(ord(vb)-96)\n",
    "                flag = True\n",
    "            \n",
    "    if features[\"Len_vowel\"]%2==0:\n",
    "        cos_result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "#         features[\"cos_lt\"] = features[\"cos_lt\"]  + cos_result\n",
    "           # Find the Cosine Distance of Last 2 Letters \n",
    "#         dataSetI = [ord(name[-2:lenName-1])-96,ord(name[0].lower())-96,] # This returns Number of Letters\n",
    "#         dataSetII = [ord(name[-1:lenName])-96,ord(name[-1].lower())-96,]  # This returns Number of Letters\n",
    "\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def gender_features_icv(name):\n",
    "    features = {}\n",
    "    temp_name = name.lower()\n",
    "    lenName = len(name)    \n",
    "    features[\"name_len\"] = len(name)\n",
    "    features[\"firstletter\"] = name[0].lower() \n",
    "    features[\"lastletter\"] = name[-1].lower() \n",
    "    features[\"prefix\"] = name[:3].lower() if len(name) > 4 else name[:2].lower() \n",
    "    features[\"suffix\"] = name[-3:].lower() if len(name) > 4 else name[-2:].lower()\n",
    "    Vowel = ['a','e','i','o','u']\n",
    "    Vclusters = []\n",
    "    dataSetI = []\n",
    "    dataSetII = []\n",
    "#     Check if Last 2 Letters are Vowels\n",
    "#     if name[-2:lenName-1] in Vowel :  \n",
    "#         features[\"Vowel_l2\"] =  1\n",
    "#     else :  \n",
    "#         features[\"Vowel_l2\"] =  0\n",
    "#     if name[-1:lenName] in Vowel : \n",
    "#         features[\"Vowel_l1\"] =  1 \n",
    "#     else :\n",
    "#         features[\"Vowel_l1\"] =  0\n",
    "#     if lenName >= 4 :    else:       \n",
    "    cos_result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "   \n",
    "    features[\"Len_vowel\"] = 0  \n",
    "    dataSetI = []\n",
    "    dataSetII = []\n",
    "    flag = False\n",
    "    for vb in Vowel[::-1]:\n",
    "        if vb in temp_name:\n",
    "            n_vowels= temp_name.count(vb) # COunt how many times you see Vowels            \n",
    "            temp_name = temp_name.replace(vb, \"\")\n",
    "            Vclusters.append(vb)\n",
    "            features[vb]=n_vowels\n",
    "#             features[\"Len_vowel\"] = features[\"Len_vowel\"] + n_vowels\n",
    "            if flag == True:\n",
    "                dataSetI.append(ord(vb)-96)\n",
    "                flag = False\n",
    "            else :\n",
    "                dataSetII.append(ord(vb)-96)\n",
    "                flag = True\n",
    "#     features[\"Len_Ratio\"] = (lenName - features[\"Len_vowel\"])\n",
    "    #Check if There are two letters repeated next to each other\n",
    "    ch = [name[i]==name[i+1] for i in range(len(name)) if i <= len(name)-2 ]\n",
    "#     features[\"Double\"] = ch.count(True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name_len': 8,\n",
       " 'firstletter': 'r',\n",
       " 'lastletter': 'a',\n",
       " 'prefix': 'rai',\n",
       " 'suffix': 'ama',\n",
       " 'Len_vowel': 0,\n",
       " 'i': 2,\n",
       " 'a': 3}"
      ]
     },
     "execution_count": 1283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier_icv.classify(gender_features_icv(\"Rajendra\"))\n",
    "# gender_features_icv(\"Raj\")\n",
    "temp_name = \"Rajiity\"\n",
    "\n",
    "\n",
    "# dataSetI = [ord(temp_name[-2:len(temp_name)-1])-96] # This returns Number of Letters\n",
    "# dataSetII = [ord(temp_name[-1:len(temp_name)])-96]  # This returns Number of Letters\n",
    "# cos_result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "\n",
    "# cos_result\n",
    "\n",
    "# # result\n",
    "# # temp_name[-2:len(temp_name)-1],temp_name[-1:len(temp_name)]\n",
    "\n",
    "gender_features_icv(\"Rajama\")\n",
    "[temp_name[a] for a in range(len(temp_name))]\n",
    "[(temp_name[a],temp_name[b]) if a <= len(temp_name) else b - 1 \n",
    " for a in range(len(temp_name))  for b in range(len(temp_name[a:a+2]))   ]\n",
    "\n",
    "for i in range(len(temp_name)):\n",
    "    if i <= len(temp_name)-2:\n",
    "        print(temp_name[i]==temp_name[i+1])\n",
    "\n",
    "# len(temp_name),temp_name,temp_name[4]\n",
    "\n",
    "ch = [temp_name[i]==temp_name[i+1] for i in range(len(temp_name)) if i <= len(temp_name)-2 ]\n",
    "ch.count(True)\n",
    "\n",
    "gender_features_icv(\"Raiijama\")\n",
    "# ch = [temp_name[i]==temp_name[i+1] for i in range(len(temp_name)) if i <= len(temp_name)-2 ]\n",
    "# ch.count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features_icv(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features_icv(n), g) for (n,g) in devtest_names]\n",
    "classifier_icv = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier_icv, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              lastletter = 'a'            female : male   =     31.2 : 1.0\n",
      "              lastletter = 'k'              male : female =     27.2 : 1.0\n",
      "                  suffix = 'ard'            male : female =     22.6 : 1.0\n",
      "                  suffix = 'tta'          female : male   =     19.6 : 1.0\n",
      "              lastletter = 'v'              male : female =     18.5 : 1.0\n",
      "                  suffix = 'na'           female : male   =     17.4 : 1.0\n",
      "                  suffix = 'nne'          female : male   =     17.2 : 1.0\n",
      "              lastletter = 'f'              male : female =     15.2 : 1.0\n",
      "                  prefix = 'rod'            male : female =     14.9 : 1.0\n",
      "                  suffix = 'vin'            male : female =     13.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_icv.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=male     guess=female   name=Abby                          \n",
      "correct=male     guess=female   name=Abner                         \n",
      "correct=male     guess=female   name=Adam                          \n",
      "correct=male     guess=female   name=Adlai                         \n",
      "correct=male     guess=female   name=Aguste                        \n",
      "correct=male     guess=female   name=Albrecht                      \n",
      "correct=male     guess=female   name=Alec                          \n",
      "correct=male     guess=female   name=Aleksandrs                    \n",
      "correct=male     guess=female   name=Allen                         \n",
      "correct=male     guess=female   name=Allie                         \n",
      "correct=male     guess=female   name=Ambrose                       \n",
      "correct=male     guess=female   name=Ambrosi                       \n",
      "correct=male     guess=female   name=Ambrosius                     \n",
      "correct=male     guess=female   name=Amos                          \n",
      "correct=male     guess=female   name=Andrej                        \n",
      "correct=male     guess=female   name=Andres                        \n",
      "correct=male     guess=female   name=Angelico                      \n",
      "correct=male     guess=female   name=Angelo                        \n",
      "correct=male     guess=female   name=Anton                         \n",
      "correct=male     guess=female   name=Antonin                       \n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier_tls.classify(gender_features_icv(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )\n",
    "\n",
    "errors = sorted(errors)\n",
    "\n",
    "for (tag, guess, name) in errors[0:20]: # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    print ('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Last Letter  [classifier] check :0.7666163141993958\n",
      "Accuracy First and Last Letter [classifier_az] check :0.771399798590131\n",
      "Accuracy First and Last Letter and last 2 letter [classifier_4] check :0.8272910372608258\n",
      "Accuracy Last Two Letter [classifier_tls] check :0.7812185297079557\n",
      "Accuracy With All+ Vowels [classifier_icv] check :0.8267875125881168\n"
     ]
    }
   ],
   "source": [
    "# MOVE TO END \n",
    "from sklearn.model_selection import train_test_split\n",
    "base_data = [(n, g) for (n,g) in Gender_names]\n",
    "# split data into training and test data.\n",
    "gl_train_set, gl_test_set = train_test_split(base_data,train_size=0.5,test_size=0.5,shuffle =True)\n",
    "\n",
    "featuresets_train = [(gender_features(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_test = [(gender_features(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "featuresets_az_train = [(gender_features_az(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_az_test = [(gender_features_az(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "featuresets_tls_train = [(gender_features_tls(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_tls_test = [(gender_features_tls(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "featuresets_4_train = [(class_gender_features4(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_4_test = [(class_gender_features4(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "featuresets_icv_train = [(gender_features_icv(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_icv_test = [(gender_features_icv(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(featuresets_train)\n",
    "classifier_az = nltk.NaiveBayesClassifier.train(featuresets_az_train)\n",
    "classifier_4 = nltk.NaiveBayesClassifier.train(featuresets_4_train)\n",
    "classifier_tls = nltk.NaiveBayesClassifier.train(featuresets_tls_train)\n",
    "classifier_icv = nltk.NaiveBayesClassifier.train(featuresets_icv_train)\n",
    "\n",
    "\n",
    "\n",
    "accuracy_base= nltk.classify.accuracy(classifier, featuresets_test)\n",
    "accuracy_az= nltk.classify.accuracy(classifier_az, featuresets_az_test)\n",
    "accuracy_4= nltk.classify.accuracy(classifier_4, featuresets_4_test)\n",
    "accuracy_tls= nltk.classify.accuracy(classifier_tls, featuresets_tls_test)\n",
    "accuracy_icv= nltk.classify.accuracy(classifier_icv, featuresets_icv_test)\n",
    "\n",
    "print (\"Accuracy with Last Letter  [classifier] check :{}\".format(accuracy_base))\n",
    "print (\"Accuracy First and Last Letter [classifier_az] check :{}\".format(accuracy_az))\n",
    "print (\"Accuracy First and Last Letter and last 2 letter [classifier_4] check :{}\".format(accuracy_4))\n",
    "print (\"Accuracy Last Two Letter [classifier_tls] check :{}\".format(accuracy_tls))\n",
    "print (\"Accuracy With All+ Vowels [classifier_icv] check :{}\".format(accuracy_icv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features_tls(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features_tls(n), g) for (n,g) in devtest_names]\n",
    "classifier_tls = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier_tls, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing accuracy:\n",
    "We will test the accuracy of of both gender features of finding the gender by last name and counting the letters of names here. To do this, we will run each function 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(number_of_runs, function_to_use):\n",
    "    acc_df = {\n",
    "        \"classifier\": [],\n",
    "        \"train_set_accuracy\": [],\n",
    "        \"test_set_accuracy\": [],\n",
    "        \"devtest_set_accuracy\": [],\n",
    "        \"devtest_errors\": []\n",
    "    }\n",
    "    for i in range(number_of_runs):\n",
    "        random.shuffle(Gender_names)\n",
    "        acc_train_names = Gender_names[1000:]\n",
    "        acc_devtest_names = Gender_names[500:1000]\n",
    "        acc_test_names = Gender_names[:500]\n",
    "        acc_train_set = [(function_to_use(n), g) for (n,g) in acc_train_names]\n",
    "        acc_devtest_set = [(function_to_use(n), g) for (n,g) in acc_devtest_names]\n",
    "        acc_test_set = [(function_to_use(n), g) for (n,g) in acc_test_names]\n",
    "        acc_classifier = nltk.NaiveBayesClassifier.train(acc_train_set)\n",
    "        acc_df[\"classifier\"].append(acc_classifier)\n",
    "        acc_df[\"train_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_train_set))\n",
    "        acc_df[\"test_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_test_set))\n",
    "        acc_df[\"devtest_set_accuracy\"].append(nltk.classify.accuracy(acc_classifier, acc_devtest_set))\n",
    "        acc_errors = []\n",
    "        for (name, tag) in acc_devtest_names:\n",
    "            acc_guess = acc_classifier.classify(function_to_use(name))\n",
    "            if acc_guess != tag:\n",
    "                acc_errors.append( (tag, acc_guess, name) )\n",
    "        acc_df[\"devtest_errors\"].append(acc_errors)\n",
    "    acc_df = pd.DataFrame.from_dict(acc_df)\n",
    "    return(acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_set_accuracy</th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>devtest_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.763174</td>\n",
       "      <td>0.759260</td>\n",
       "      <td>0.759660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.018119</td>\n",
       "      <td>0.017762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.759361</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.761953</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.763105</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.764185</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.767137</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_set_accuracy  test_set_accuracy  devtest_set_accuracy\n",
       "count          100.000000         100.000000            100.000000\n",
       "mean             0.763174           0.759260              0.759660\n",
       "std              0.001712           0.018119              0.017762\n",
       "min              0.759361           0.720000              0.716000\n",
       "25%              0.761953           0.748000              0.750000\n",
       "50%              0.763105           0.757000              0.759000\n",
       "75%              0.764185           0.772000              0.774000\n",
       "max              0.767137           0.808000              0.810000"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_df_1 = accuracy(100, gender_features)\n",
    "Accuracy_df_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the first features shows that the average (mean) accuracy accross the test_set are between 78.1% and 78.9%. The mean accuracy of train_set is more than the accuracy of devtest_set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_set_accuracy</th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>devtest_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.778135</td>\n",
       "      <td>0.775200</td>\n",
       "      <td>0.774260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.017805</td>\n",
       "      <td>0.015598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.773474</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.776642</td>\n",
       "      <td>0.765500</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.778370</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.779414</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.783266</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.816000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_set_accuracy  test_set_accuracy  devtest_set_accuracy\n",
       "count          100.000000         100.000000            100.000000\n",
       "mean             0.778135           0.775200              0.774260\n",
       "std              0.002122           0.017805              0.015598\n",
       "min              0.773474           0.730000              0.738000\n",
       "25%              0.776642           0.765500              0.764000\n",
       "50%              0.778370           0.777000              0.774000\n",
       "75%              0.779414           0.786000              0.784500\n",
       "max              0.783266           0.824000              0.816000"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_df_2 = accuracy(100, gender_features_az)\n",
    "Accuracy_df_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the second features shows that the average (mean) accuracy across the test_set are between 77.3% and 77.9%. The mean accuracy of train_set is more than the accuracy of devtest_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_set_accuracy</th>\n",
       "      <th>test_set_accuracy</th>\n",
       "      <th>devtest_set_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.883338</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.009615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.881192</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.882380</td>\n",
       "      <td>0.828500</td>\n",
       "      <td>0.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.883137</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.884649</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.885513</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_set_accuracy  test_set_accuracy  devtest_set_accuracy\n",
       "count           10.000000          10.000000             10.000000\n",
       "mean             0.883338           0.834600              0.824000\n",
       "std              0.001471           0.007834              0.009615\n",
       "min              0.881192           0.824000              0.808000\n",
       "25%              0.882380           0.828500              0.820500\n",
       "50%              0.883137           0.836000              0.824000\n",
       "75%              0.884649           0.837500              0.831500\n",
       "max              0.885513           0.852000              0.838000"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_df_3 = accuracy(10, class_gender_features4)\n",
    "class_df_3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the third features, looking for prefix and suffix in the names, shows that the average (mean) accuracy across the test_set are between 83.3% and 88.4%. The mean accuracy of train_set is more than the accuracy of devtest_set and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW MODEL\n",
    "gender_feature_icv(\"temp_name\")\n",
    "features = {}\n",
    "temp_name = \"Rajwanti\"\n",
    "cons_clusters = [\"bl\", \"br\", \"ch\", \"cl\", \"cr\", \"dr\", \"fl\", \"fr\", \n",
    "                     \"gl\", \"gr\", \"pl\", \"pr\", \"sc\", \"sh\", \"sk\", \"sl\", \n",
    "                     \"sm\", \"sn\", \"sp\", \"st\", \"sw\", \"th\", \"tr\", \"tw\", \n",
    "                     \"wh\", \"wr\", \"sch\", \"scr\", \"shr\", \"sph\", \"spl\", \n",
    "                     \"spr\", \"squ\", \"str\", \"thr\"]\n",
    "features[\"firstletter\"] = name[0].lower() \n",
    "features[\"lastletter\"] = name[-1].lower() \n",
    "features[\"prefix\"] = name[:3].lower() if len(name) > 4 else name[:2].lower() \n",
    "features[\"suffix\"] = name[-3:].lower() if len(name) > 4 else name[-2:].lower()\n",
    "# clusters = []\n",
    "# for cluster in cons_clusters[::-1]:\n",
    "#     if cluster in temp_name:\n",
    "#         temp_name = temp_name.replace(cluster, \"\")\n",
    "#         clusters.append(cluster)\n",
    "# features[\"consonant_clusters_1\"] = clusters[0] if len(clusters) > 0 else None\n",
    "# features[\"consonant_clusters_2\"] = clusters[1] if len(clusters) > 1 else None\n",
    "# features[\"consonant_clusters_3\"] = clusters[2] if len(clusters) > 2 else None\n",
    "# len(temp_name)\n",
    "Vowel = ['a','e','i','o','u']\n",
    "Vclusters = []\n",
    "for vb in Vowel[::-1]:\n",
    "    if vb in temp_name:\n",
    "        temp_name = temp_name.replace(vb, \"\")\n",
    "        Vclusters.append(vb)\n",
    "features[\"Len_vowel\"] = len(Vclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters,Vclusters, features\n",
    "len(temp_name)\n",
    "Vowel = ['a','e','i','o','u']\n",
    "Vclusters = []\n",
    "for vb in Vowel[::-1]:\n",
    "    if vb in temp_name:\n",
    "        temp_name.count(vb)\n",
    "        temp_name = temp_name.replace(vb, \"\")\n",
    "        Vclusters.append(vb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-865-d95ec52c4f2c>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-865-d95ec52c4f2c>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    if Vowel in \"Rajwant\"\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gender_feature_icv(\"Rajwant\")\n",
    "\n",
    "# 'a' in \"Rajwant\"\n",
    "\n",
    "# \"Rajwant\".count('a')\n",
    "\n",
    "if Vowel in \"Rajwant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Last Letter  [classifier] check :0.7640986908358509\n",
      "Accuracy First and Last Letter [classifier_az] check :0.7648539778449144\n",
      "Accuracy First and Last Letter and last 2 letter [classifier_4] check :0.82527693856999\n",
      "Accuracy Last Two Letter [classifier_tls] check :0.7842396777442094\n",
      "Accuracy With All+ Vowels [classifier_icv] check :0.81797583081571\n"
     ]
    }
   ],
   "source": [
    "# MOVE TO END \n",
    "from sklearn.model_selection import train_test_split\n",
    "base_data = [(n, g) for (n,g) in Gender_names]\n",
    "# split data into training and test data.\n",
    "gl_train_set, gl_test_set = train_test_split(base_data,train_size=0.5,test_size=0.5,shuffle =True)\n",
    "\n",
    "featuresets_train = [(gender_features(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_test = [(gender_features(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "featuresets_az_train = [(gender_features_az(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_az_test = [(gender_features_az(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "featuresets_tls_train = [(gender_features_tls(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_tls_test = [(gender_features_tls(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "featuresets_4_train = [(class_gender_features4(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_4_test = [(class_gender_features4(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "featuresets_icv_train = [(gender_features_icv(n), g) for (n,g) in gl_train_set]\n",
    "featuresets_icv_test = [(gender_features_icv(n), g) for (n,g) in gl_test_set]\n",
    "\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(featuresets_train)\n",
    "classifier_az = nltk.NaiveBayesClassifier.train(featuresets_az_train)\n",
    "classifier_4 = nltk.NaiveBayesClassifier.train(featuresets_4_train)\n",
    "classifier_tls = nltk.NaiveBayesClassifier.train(featuresets_tls_train)\n",
    "classifier_icv = nltk.NaiveBayesClassifier.train(featuresets_icv_train)\n",
    "\n",
    "accuracy_base= nltk.classify.accuracy(classifier, featuresets_test)\n",
    "accuracy_az= nltk.classify.accuracy(classifier_az, featuresets_az_test)\n",
    "accuracy_4= nltk.classify.accuracy(classifier_4, featuresets_4_test)\n",
    "accuracy_tls= nltk.classify.accuracy(classifier_tls, featuresets_tls_test)\n",
    "accuracy_icv= nltk.classify.accuracy(classifier_icv, featuresets_icv_test)\n",
    "\n",
    "print (\"Accuracy with Last Letter  [classifier] check :{}\".format(accuracy_base))\n",
    "print (\"Accuracy First and Last Letter [classifier_az] check :{}\".format(accuracy_az))\n",
    "print (\"Accuracy First and Last Letter and last 2 letter [classifier_4] check :{}\".format(accuracy_4))\n",
    "print (\"Accuracy Last Two Letter [classifier_tls] check :{}\".format(accuracy_tls))\n",
    "print (\"Accuracy With All+ Vowels [classifier_icv] check :{}\".format(accuracy_icv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
